{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNXxh/Wm/A9BNv2GeYVLiSG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminnigjeh/automated_data_mining/blob/main/torch_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install additional libraries"
      ],
      "metadata": {
        "id": "XBjdzBrAYON9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip Install wget\n",
        "!pip install xlstm"
      ],
      "metadata": {
        "id": "UpqtAXppYNhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import external libraries"
      ],
      "metadata": {
        "id": "mtRjc3wKYfbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xlstm\n",
        "import wget\n",
        "import h5py as h5\n",
        "from random import shuffle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xlstm import (\n",
        "    xLSTMBlockStack,\n",
        "    xLSTMBlockStackConfig,\n",
        "    mLSTMBlockConfig,\n",
        "    mLSTMLayerConfig,\n",
        "    sLSTMBlockConfig,\n",
        "    sLSTMLayerConfig,\n",
        "    FeedForwardConfig,\n",
        ")\n"
      ],
      "metadata": {
        "id": "F_-n7uVhDOdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download the raw dataset"
      ],
      "metadata": {
        "id": "klj5ZBItZXre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "url = 'https://figshare.com/ndownloader/files/12506534'\n",
        "wget.download(url)"
      ],
      "metadata": {
        "id": "cPmDyi6cZChi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare training and test dataset compatible with pytorch"
      ],
      "metadata": {
        "id": "aUhYqGM5ZmLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the constants\n",
        "PROSIT_ALHABET = {\"A\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6, \"H\": 7, \"I\": 8, \"K\": 9, \"L\": 10, \"M\": 11, \"N\": 12, \"P\": 13, \"Q\": 14, \"R\": 15, \"S\": 16, \"T\": 17, \"V\": 18, \"W\": 19, \"Y\": 20, \"M(ox)\": 21}\n",
        "PROSIT_INDEXED_ALPHABET = {i: c for c, i in PROSIT_ALHABET.items()}\n",
        "\n",
        "# Read from the dowloaded raw dataset\n",
        "with h5.File('/content/holdout_hcd.hdf5', 'r') as f:\n",
        "  KEY_ARRAY = [\"sequence_integer\", \"precursor_charge_onehot\", \"intensities_raw\"]\n",
        "  KEY_SCALAR = [\"collision_energy_aligned_normed\", \"collision_energy\"]\n",
        "  df = pd.DataFrame({key: list(f[key][...]) for key in KEY_ARRAY})\n",
        "  for key in KEY_SCALAR:\n",
        "    df[key] = f[key][...]\n",
        "\n",
        "# Add convenience columns\n",
        "df['precursor_charge'] = df.precursor_charge_onehot.map(lambda a: a.argmax() + 1)\n",
        "df['sequence_maxquant'] = df.sequence_integer.map(lambda s: \"\".join(PROSIT_INDEXED_ALPHABET[i] for i in s if i != 0))\n",
        "df['sequence_length'] = df.sequence_integer.map(lambda s: np.count_nonzero(s))\n",
        "\n",
        "# Generate unique sequences\n",
        "def unique_dataframe(df, unique_column):\n",
        "\n",
        "    unique = list(set(df[unique_column]))\n",
        "    n_unique = len(unique)\n",
        "    shuffle(unique)\n",
        "    unique_train = unique\n",
        "    df_train = df[df[unique_column].isin(unique_train)]\n",
        "    assert len(df_train) == len(df)\n",
        "    return df_train\n",
        "\n",
        "df_train = unique_dataframe(df, unique_column='sequence_maxquant')\n",
        "\n",
        "\n",
        "# Prepare the training dataset in numpy\n",
        "INPUT_COLUMNS = ('sequence_integer', 'precursor_charge_onehot', 'collision_energy_aligned_normed')\n",
        "OUTPUT_COLUMN = 'intensities_raw'\n",
        "\n",
        "x_train = [np.vstack(df_train[column]) for column in INPUT_COLUMNS]\n",
        "y_train = np.vstack(df_train[OUTPUT_COLUMN])\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 256\n",
        "\n",
        "# Convert numpy arrays to torch tensors\n",
        "input1 = torch.tensor(x_train[0], dtype=torch.int64)\n",
        "input2 = torch.tensor(x_train[1], dtype=torch.float32)\n",
        "input3 = torch.tensor(x_train[2], dtype=torch.float32)\n",
        "labels = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "# Move the data to the CUDA device\n",
        "input1, input2, input3, labels = input1.to(device), input2.to(device), input3.to(device), labels.to(device)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_input1, test_input1, train_input2, test_input2, train_input3, test_input3, train_labels, test_labels = train_test_split(\n",
        "    input1, input2, input3, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Generate dataLoader for batching\n",
        "train_dataset = torch.utils.data.TensorDataset(train_input1, train_input2, train_input3, train_labels)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(test_input1, test_input2, test_input3, test_labels)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "IDzEg2HIY-sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer encoder architecture"
      ],
      "metadata": {
        "id": "3_QrUKjSaWDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Transformer Encoder Model\n",
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerEncoderModel, self).__init__()\n",
        "\n",
        "        # Update embedding layer for input1 to handle values from 0 to 21 and output 32-dimensional embeddings\n",
        "        self.embedding_input1 = nn.Embedding(22, 32)  # 22 because the values range from 0 to 21\n",
        "\n",
        "        # Transformer encoder for the first input (32-dimensional sequence), batch_first=True\n",
        "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=32, nhead=2, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=2)\n",
        "\n",
        "        # Fully connected layer for the first input (32-dimensional sequence)\n",
        "        self.fc_input1 = nn.Linear(32, 124)\n",
        "\n",
        "        # Fully connected layer for the second input (6-dimensional sequence)\n",
        "        self.fc_input2 = nn.Linear(6, 124)\n",
        "\n",
        "        # Fully connected layer for the third input (1-dimensional sequence)\n",
        "        self.fc_input3 = nn.Linear(1, 124)\n",
        "\n",
        "        # Fully connected layer after multiplication of both inputs\n",
        "        self.fc_after_multiplication = nn.Linear(124, 124)\n",
        "\n",
        "        # Final output layer to produce a 174-dimensional output\n",
        "        self.fc_output = nn.Linear(124, 174)\n",
        "\n",
        "    def forward(self, input1, input2, input3):\n",
        "        # Process the first input through the embedding layer\n",
        "        input1 = self.embedding_input1(input1.long())  # Ensure input1 is long for embedding\n",
        "\n",
        "        # Pass the embedded input1 through the transformer encoder\n",
        "        input1 = self.transformer_encoder(input1)  # [batch_size, seq_len, feature_dim]\n",
        "\n",
        "        # Reduce input1 over the sequence length dimension (e.g., take mean)\n",
        "        input1 = input1.sum(dim=1)  # Now shape is [batch_size, feature_dim]\n",
        "\n",
        "        # Process the output of the transformer through the fully connected layer\n",
        "        input1 = self.fc_input1(input1)\n",
        "\n",
        "        # Process the second input through the fully connected layer\n",
        "        input2 = self.fc_input2(input2)\n",
        "\n",
        "        # Process the third input through the fully connected layer\n",
        "        input3 = self.fc_input3(input3)\n",
        "\n",
        "        # Multiply the two outputs element-wise\n",
        "        combined = input1 * input2 * input3\n",
        "\n",
        "        # Pass the result through a fully connected layer\n",
        "        combined = F.relu(self.fc_after_multiplication(combined))\n",
        "\n",
        "        # Final output layer to produce a 174-dimensional output\n",
        "        output = self.fc_output(combined)\n",
        "\n",
        "        return output\n",
        "\n",
        "def masked_spectral_distance(true, pred):\n",
        "    \"\"\"\n",
        "    Calculate the masked spectral distance (similar to cosine distance) in PyTorch.\n",
        "    \"\"\"\n",
        "    epsilon = 1e-8  # Small value to avoid division by zero\n",
        "\n",
        "    # Masked predictions and true values (similar to the formula you provided)\n",
        "    pred_masked = ((true + 1) * pred) / (true + 1 + epsilon)\n",
        "    true_masked = ((true + 1) * true) / (true + 1 + epsilon)\n",
        "\n",
        "    # Normalize both predicted and true masked tensors\n",
        "    pred_norm = F.normalize(pred_masked, p=2, dim=-1)\n",
        "    true_norm = F.normalize(true_masked, p=2, dim=-1)\n",
        "\n",
        "    # Compute the cosine similarity (dot product)\n",
        "    product = torch.sum(pred_norm * true_norm, dim=1)\n",
        "\n",
        "    # Compute arccosine of the product (clip to avoid values outside the valid range for acos)\n",
        "    product = torch.clamp(product, min=-1.0, max=1.0)\n",
        "    arccos = torch.acos(product)\n",
        "\n",
        "    # Return the normalized distance\n",
        "    return 2 * arccos / np.pi"
      ],
      "metadata": {
        "id": "lHzR5FnIC2Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## xLSTM encoder architecture"
      ],
      "metadata": {
        "id": "HZ5AwL_pajwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class xLSTMEncoderModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(xLSTMEncoderModel, self).__init__()\n",
        "\n",
        "        # Update embedding layer for input1 to handle values from 0 to 21 and output 32-dimensional embeddings\n",
        "        self.embedding_input1 = nn.Embedding(22, 32)  # 22 because the values range from 0 to 21\n",
        "\n",
        "        # xLSTM stack configuration\n",
        "        cfg = xLSTMBlockStackConfig(\n",
        "        mlstm_block=mLSTMBlockConfig(\n",
        "        mlstm=mLSTMLayerConfig(\n",
        "            conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n",
        "        )\n",
        "        ),\n",
        "        slstm_block=sLSTMBlockConfig(\n",
        "        slstm=sLSTMLayerConfig(\n",
        "            backend=\"cuda\",\n",
        "            num_heads=4,\n",
        "            conv1d_kernel_size=4,\n",
        "            bias_init=\"powerlaw_blockdependent\",\n",
        "            ),\n",
        "            feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\"),\n",
        "            ),\n",
        "            context_length=256,\n",
        "            num_blocks=7,\n",
        "            embedding_dim=32,  # Change embedding_dim to 32 to match the embedding output size\n",
        "            slstm_at=[1],\n",
        "            )\n",
        "\n",
        "        # Instantiate the xLSTM block stack\n",
        "        self.xlstm_stack = xLSTMBlockStack(cfg)\n",
        "\n",
        "        # Fully connected layer for the first input (32-dimensional sequence)\n",
        "        self.fc_input1 = nn.Linear(32, 124)\n",
        "\n",
        "        # Fully connected layer for the second input (6-dimensional sequence)\n",
        "        self.fc_input2 = nn.Linear(6, 124)\n",
        "\n",
        "        # Fully connected layer for the third input (1-dimensional sequence)\n",
        "        self.fc_input3 = nn.Linear(1, 124)\n",
        "\n",
        "        # Fully connected layer after multiplication of both inputs\n",
        "        self.fc_after_multiplication = nn.Linear(124, 124)\n",
        "\n",
        "        # Final output layer to produce a 174-dimensional output\n",
        "        self.fc_output = nn.Linear(124, 174)\n",
        "\n",
        "    def forward(self, input1, input2, input3):\n",
        "        # Process the first input through the embedding layer\n",
        "        input1 = self.embedding_input1(input1.long())  # Ensure input1 is long for embedding\n",
        "\n",
        "        # Pass the embedded input1 through the xLSTM stack\n",
        "        # xLSTM expects [batch_size, seq_len, feature_dim]\n",
        "        input1 = self.xlstm_stack(input1)  # [batch_size, seq_len, feature_dim]\n",
        "\n",
        "        # Reduce input1 over the sequence length dimension (e.g., take sum)\n",
        "        input1 = input1.sum(dim=1)  # Now shape is [batch_size, feature_dim]\n",
        "\n",
        "        # Process the output of the xLSTM through the fully connected layer\n",
        "        input1 = self.fc_input1(input1)\n",
        "\n",
        "        # Process the second input through the fully connected layer\n",
        "        input2 = self.fc_input2(input2)\n",
        "\n",
        "        # Process the third input through the fully connected layer\n",
        "        input3 = self.fc_input3(input3)\n",
        "\n",
        "        # Multiply the two outputs element-wise\n",
        "        combined = input1 * input2 * input3\n",
        "\n",
        "        # Pass the result through a fully connected layer\n",
        "        combined = F.relu(self.fc_after_multiplication(combined))\n",
        "\n",
        "        # Final output layer to produce a 174-dimensional output\n",
        "        output = self.fc_output(combined)\n",
        "\n",
        "        return output\n",
        "def masked_spectral_distance(true, pred):\n",
        "    \"\"\"\n",
        "    Calculate the masked spectral distance (similar to cosine distance) in PyTorch.\n",
        "    \"\"\"\n",
        "    epsilon = 1e-8  # Small value to avoid division by zero\n",
        "\n",
        "    # Masked predictions and true values (similar to the formula you provided)\n",
        "    pred_masked = ((true + 1) * pred) / (true + 1 + epsilon)\n",
        "    true_masked = ((true + 1) * true) / (true + 1 + epsilon)\n",
        "\n",
        "    # Normalize both predicted and true masked tensors\n",
        "    pred_norm = F.normalize(pred_masked, p=2, dim=-1)\n",
        "    true_norm = F.normalize(true_masked, p=2, dim=-1)\n",
        "\n",
        "    # Compute the cosine similarity (dot product)\n",
        "    product = torch.sum(pred_norm * true_norm, dim=1)\n",
        "\n",
        "    # Compute arccosine of the product (clip to avoid values outside the valid range for acos)\n",
        "    product = torch.clamp(product, min=-1.0, max=1.0)\n",
        "    arccos = torch.acos(product)\n",
        "\n",
        "    # Return the normalized distance\n",
        "    return 2 * arccos / np.pi"
      ],
      "metadata": {
        "id": "GedUQjvITgGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select and compile the model"
      ],
      "metadata": {
        "id": "tGRbM3doavQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_epochs = 50\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = TransformerEncoderModel().to(device)  # Move model to CUDA\n",
        "#model = xLSTMEncoderModel().to(device)  # Move model to CUDA\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f'Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n"
      ],
      "metadata": {
        "id": "JrqBv0nT_QAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "Z14uoEEia4qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []  # To store training losses per epoch\n",
        "test_losses = []   # To store evaluation losses per epoch\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    print(device)\n",
        "    running_loss = 0.0  # To accumulate the training loss for this epoch\n",
        "\n",
        "    for input1_batch, input2_batch, input3_batch, labels_batch in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        # Move batches to CUDA (GPU) if available\n",
        "        input1_batch, input2_batch, input3_batch, labels_batch = input1_batch.to(device), input2_batch.to(device), input3_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input1_batch, input2_batch, input3_batch)\n",
        "\n",
        "        # Compute the loss using the masked spectral distance\n",
        "        loss = masked_spectral_distance(labels_batch, outputs)\n",
        "\n",
        "        # Accumulate the loss for the current batch\n",
        "        running_loss += loss.mean().item()\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.mean().backward()  # Backpropagate the mean loss\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate the average training loss for the entire epoch\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)  # Store the average training loss for this epoch\n",
        "\n",
        "    # Now evaluate the model on the test set (validation set)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_test_loss = 0.0  # To accumulate the test loss\n",
        "\n",
        "    with torch.no_grad():  # No need to compute gradients for evaluation\n",
        "        for input1_batch, input2_batch, input3_batch, labels_batch in test_loader:\n",
        "            # Move batches to CUDA (GPU) if available\n",
        "            input1_batch, input2_batch, input3_batch, labels_batch = input1_batch.to(device), input2_batch.to(device), input3_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input1_batch, input2_batch, input3_batch)\n",
        "\n",
        "            # Compute the loss using the masked spectral distance\n",
        "            loss = masked_spectral_distance(labels_batch, outputs)\n",
        "\n",
        "            # Accumulate the test loss for the current batch\n",
        "            running_test_loss += loss.mean().item()\n",
        "\n",
        "    # Calculate the average test loss for the entire epoch\n",
        "    avg_test_loss = running_test_loss / len(test_loader)\n",
        "    test_losses.append(avg_test_loss)  # Store the average test loss for this epoch\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "# Plotting the losses per epoch\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss', color='b')\n",
        "plt.plot(range(1, num_epochs+1), test_losses, label='Validation Loss', color='r')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss per Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PgUdO2Tsaxn9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}